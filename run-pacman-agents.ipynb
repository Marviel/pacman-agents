{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Rendering OpenAi Gym in Colaboratory.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "private_outputs": true,
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "GsSGgH2Y0apl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#          _____                _____                    _____                    _____                    _____                    _____          \n",
        "#         /\\    \\              /\\    \\                  /\\    \\                  /\\    \\                  /\\    \\                  /\\    \\         \n",
        "#        /::\\    \\            /::\\    \\                /::\\    \\                /::\\    \\                /::\\    \\                /::\\    \\        \n",
        "#       /::::\\    \\           \\:::\\    \\              /::::\\    \\              /::::\\    \\              /::::\\    \\               \\:::\\    \\       \n",
        "#      /::::::\\    \\           \\:::\\    \\            /::::::\\    \\            /::::::\\    \\            /::::::\\    \\               \\:::\\    \\      \n",
        "#     /:::/\\:::\\    \\           \\:::\\    \\          /:::/\\:::\\    \\          /:::/\\:::\\    \\          /:::/\\:::\\    \\               \\:::\\    \\     \n",
        "#    /:::/__\\:::\\    \\           \\:::\\    \\        /:::/__\\:::\\    \\        /:::/__\\:::\\    \\        /:::/__\\:::\\    \\               \\:::\\    \\    \n",
        "#    \\:::\\   \\:::\\    \\          /::::\\    \\      /::::\\   \\:::\\    \\      /::::\\   \\:::\\    \\      /::::\\   \\:::\\    \\              /::::\\    \\   \n",
        "#  ___\\:::\\   \\:::\\    \\        /::::::\\    \\    /::::::\\   \\:::\\    \\    /::::::\\   \\:::\\    \\    /::::::\\   \\:::\\    \\    ____    /::::::\\    \\  \n",
        "# /\\   \\:::\\   \\:::\\    \\      /:::/\\:::\\    \\  /:::/\\:::\\   \\:::\\    \\  /:::/\\:::\\   \\:::\\____\\  /:::/\\:::\\   \\:::\\    \\  /\\   \\  /:::/\\:::\\    \\ \n",
        "#/::\\   \\:::\\   \\:::\\____\\    /:::/  \\:::\\____\\/:::/  \\:::\\   \\:::\\____\\/:::/  \\:::\\   \\:::|    |/:::/  \\:::\\   \\:::\\____\\/::\\   \\/:::/  \\:::\\____\\\n",
        "#\\:::\\   \\:::\\   \\::/    /   /:::/    \\::/    /\\::/    \\:::\\  /:::/    /\\::/   |::::\\  /:::|____|\\::/    \\:::\\  /:::/    /\\:::\\  /:::/    \\::/    /\n",
        "# \\:::\\   \\:::\\   \\/____/   /:::/    / \\/____/  \\/____/ \\:::\\/:::/    /  \\/____|:::::\\/:::/    /  \\/____/ \\:::\\/:::/    /  \\:::\\/:::/    / \\/____/ \n",
        "#  \\:::\\   \\:::\\    \\      /:::/    /                    \\::::::/    /         |:::::::::/    /            \\::::::/    /    \\::::::/    /          \n",
        "#   \\:::\\   \\:::\\____\\    /:::/    /                      \\::::/    /          |::|\\::::/    /              \\::::/    /      \\::::/____/           \n",
        "#    \\:::\\  /:::/    /    \\::/    /                       /:::/    /           |::| \\::/____/               /:::/    /        \\:::\\    \\           \n",
        "#     \\:::\\/:::/    /      \\/____/                       /:::/    /            |::|  ~|                    /:::/    /          \\:::\\    \\          \n",
        "#      \\::::::/    /                                    /:::/    /             |::|   |                   /:::/    /            \\:::\\    \\         \n",
        "#       \\::::/    /                                    /:::/    /              \\::|   |                  /:::/    /              \\:::\\____\\        \n",
        "#        \\::/    /                                     \\::/    /                \\:|   |                  \\::/    /                \\::/    /        \n",
        "#         \\/____/                                       \\/____/                  \\|___|                   \\/____/                  \\/____/    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "odNaDE1zyrL2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# install dependancies, takes around 45 seconds\n",
        "\n",
        "Rendering Dependancies\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "8-AxnvAVyzQQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#remove \" > /dev/null 2>&1\" to see what is going on under the hood\n",
        "#!pip install gym pyvirtualdisplay > /dev/null 2>&1\n",
        "#!apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8A-1LTSH88EE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Pacman Dependancies"
      ]
    },
    {
      "metadata": {
        "id": "TCelFzWY9MBI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!apt-get update > /dev/null 2>&1\n",
        "!apt-get install cmake > /dev/null 2>&1\n",
        "!pip install --upgrade setuptools 2>&1\n",
        "!pip install ez_setup > /dev/null 2>&1\n",
        "!pip install gym[atari] > /dev/null 2>&1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "APXSx7hg19TH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Imports and Helper functions\n"
      ]
    },
    {
      "metadata": {
        "id": "pdb2JwZy4jGj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import gym\n",
        "from gym import logger as gymlogger\n",
        "from gym.wrappers import Monitor\n",
        "gymlogger.set_level(40) #error only\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import random\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import math\n",
        "import glob\n",
        "import io\n",
        "import base64\n",
        "from IPython.display import HTML\n",
        "\n",
        "from IPython import display as ipythondisplay"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nQEtc28G4niA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from pyvirtualdisplay import Display\n",
        "display = Display(visible=0, size=(1400, 900))\n",
        "display.start()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "G9UWeToN4r7D",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Utility functions to enable video recording of gym environment and displaying it\n",
        "To enable video, just do \"env = wrap_env(env)\"\"\n",
        "\"\"\"\n",
        "\n",
        "def show_video():\n",
        "  mp4list = glob.glob('video/*.mp4')\n",
        "  if len(mp4list) > 0:\n",
        "    mp4 = mp4list[0]\n",
        "    video = io.open(mp4, 'r+b').read()\n",
        "    encoded = base64.b64encode(video)\n",
        "    ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n",
        "                loop controls style=\"height: 400px;\">\n",
        "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
        "             </video>'''.format(encoded.decode('ascii'))))\n",
        "  else: \n",
        "    print(\"Could not find video\")\n",
        "    \n",
        "\n",
        "def wrap_env(env):\n",
        "  env = Monitor(env, './video', force=True)\n",
        "  return env"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "W3BGbWOu179M",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Pacman!"
      ]
    },
    {
      "metadata": {
        "id": "dGEFMfDOzLen",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "env = gym.make(\"MsPacman-v0\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7BmIlXhe9Q89",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#check out the pacman action space!\n",
        "print(env.action_space)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8nj5sjsk15IT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "observation = env.reset()\n",
        "\n",
        "while True:\n",
        "  \n",
        "    env.render()\n",
        "    \n",
        "    #your agent goes here\n",
        "    action = env.action_space.sample() \n",
        "         \n",
        "    observation, reward, done, info = env.step(action) \n",
        "    print(action)\n",
        "   \n",
        "        \n",
        "    if done: \n",
        "      break;\n",
        "            \n",
        "env.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mEn56TPZLr0T",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import gym\n",
        "import random\n",
        "import os\n",
        "import numpy as np\n",
        "from collections import deque\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense,Dropout\n",
        "from keras.optimizers import SGD,Adam \n",
        "import matplotlib.pyplot as plt \n",
        "\n",
        "\n",
        "\n",
        "class Player():\n",
        "\n",
        "  def __init__(self,state_size,action_size):\n",
        "    self.weights=\"./pacman.h5\"\n",
        "    self.state_size=state_size\n",
        "    self.action_size=action_size\n",
        "    self.memory=deque(maxlen=10000)\n",
        "    self.learning_rate=0.0002\n",
        "    self.gamma=0.95  \n",
        "    self.exploration_rate=1.0\n",
        "    self.exploration_min=0.1\n",
        "    self.exploration_decay=0.0000009  # This will be decreased from epsilon at each time step\n",
        "    self.replay_start=100\n",
        "    #self.testing_states=deque(maxlen=10000)  # A deque containing some random states which we will play on to see our average scores over episodes\n",
        "    self.model=self.build_model()\n",
        "\n",
        "  def build_model(self):\n",
        "    # Neural Network architecture\n",
        "    model = Sequential()\n",
        "    # I have to try different activation functions here.\n",
        "    model.add(Dense(128, input_dim=self.state_size, activation='relu')) # First hidden layer\n",
        "    #model.add(Dropout(0.2))\n",
        "    model.add(Dense(128, activation='relu'))                            # 2nd hidden layer\n",
        "    #model.add(Dropout(0.2))\n",
        "    model.add(Dense(self.action_size, activation='linear'))        # Output layer\n",
        "    model.summary()\n",
        "    model.compile(loss='mse', optimizer=Adam(lr=self.learning_rate))    \n",
        "\n",
        "    if os.path.isfile(self.weights):\n",
        "      model.load_weights(self.weights)\n",
        "      self.exploration_rate = self.exploration_min\n",
        "    return model\n",
        "\n",
        "  def save_model(self):\n",
        "      self.model.save_weights(self.weights)\n",
        "\n",
        "  def act(self, state):\n",
        "    if np.random.rand()<=self.exploration_rate:\n",
        "      return random.randrange(self.action_size)\n",
        "    else:\n",
        "      act_values=self.model.predict(state)\n",
        "      return np.argmax(act_values[0])\n",
        "\n",
        "  def remember(self, state, action, reward, next_state, done):\n",
        "    \n",
        "    self.memory.append((state, action, reward, next_state, done))\n",
        "    if self.exploration_rate>self.exploration_min:\n",
        "      self.exploration_rate-=self.exploration_decay\n",
        "\n",
        "    #print(\"Modified exploration rate is : {}\".format(self.exploration_rate))\n",
        "    \n",
        "\n",
        "  def replay(self, sample_batch_size):\n",
        "    if len(self.memory)<self.replay_start: # Unless memory is filled with 100 entries dont start training\n",
        "      return\n",
        "\n",
        "    x_batch,y_batch=[],[]\n",
        "    minibatch=random.sample(self.memory,min(len(self.memory),sample_batch_size))\n",
        "\n",
        "    for state,action,reward,next_state,done in minibatch:\n",
        "      y_target=self.model.predict(state)\n",
        "      y_target[0][action]=reward if done else reward+self.gamma*np.max(self.model.predict(next_state)[0])\n",
        "      x_batch.append(state[0])\n",
        "      y_batch.append(y_target[0])\n",
        "    self.model.fit(np.array(x_batch),np.array(y_batch),batch_size=len(x_batch),verbose=0)\n",
        "\n",
        "class Pacman:\n",
        "  def __init__(self):\n",
        "    self.sample_batch_size=32\n",
        "    self.episodes=10000\n",
        "    self.env=gym.make('MsPacman-ram-v0')\n",
        "    self.state_size=self.env.observation_space.shape[0]\n",
        "    self.action_size=self.env.action_space.n\n",
        "    self.player=Player(self.state_size, self.action_size)\n",
        "\n",
        "  def prepare(self):\n",
        "\n",
        "    for episode in range(50):\n",
        "\n",
        "      state=self.env.reset()\n",
        "      state=np.reshape(state,[1,self.state_size])\n",
        "\n",
        "      self.player.testing_states.append(state)\n",
        "\n",
        "      done=False\n",
        "      step=0\n",
        "      episode_reward=0\n",
        "\n",
        "      while not done:\n",
        "        action=self.player.act(state)\n",
        "        next_state,reward,done,info=self.env.step(action)\n",
        "        next_state=np.reshape(next_state, [1, self.state_size])\n",
        "        state=next_state\n",
        "        self.player.testing_states.append(state)\n",
        "\n",
        "    print(\"Done preparing testing states. Hail Mayank, the Lord of Deep Learning. :P \")\n",
        "\n",
        "  def learn(self):\n",
        "\n",
        "    try:\n",
        "      max_score=0\n",
        "\n",
        "      rewards=[]\n",
        "      testing_scores=[]\n",
        "\n",
        "      mean_rewards=[]\n",
        "      median_rewards=[]\n",
        "\n",
        "      for episode in range(self.episodes):\n",
        "\n",
        "        \n",
        "\n",
        "        # I will use a frame skip of size 4\n",
        "        frame_counter=0\n",
        "\n",
        "        state=self.env.reset()\n",
        "        state=np.reshape(state, [1, self.state_size])\n",
        "\n",
        "        done=False\n",
        "        step=0\n",
        "        episode_reward=0   # Total reward in an episode\n",
        "        #cur_lives=3 \n",
        "\n",
        "        while not done:\n",
        "          self.env.render()\n",
        "\n",
        "          if frame_counter%4==0:  # for first frame in 4 frames we predict action\n",
        "            action=self.player.act(state)\n",
        "\n",
        "\n",
        "          next_state,reward,done,info=self.env.step(action)\n",
        "\n",
        "          #if info['ale.lives']<cur_lives: # Someone made pacman dead\n",
        "          #  reward=-100                 # Get immediate reward of -100 if pacman is dead\n",
        "          #  cur_lives=info['ale.lives']  # Decrease cur_lives\n",
        "\n",
        "  \n",
        "          episode_reward+=reward\n",
        "\n",
        "          next_state=np.reshape(next_state, [1, self.state_size])\n",
        "\n",
        "          if frame_counter%4==3: # Only for last frame we remember the action\n",
        "            self.player.remember(state, action, reward, next_state, done)\n",
        "            \n",
        "          if done:  # If pacman becomes dead it is necessary to remember this. \n",
        "            self.player.remember(state, action, reward, next_state, done)\n",
        "            \n",
        "\n",
        "          state=next_state\n",
        "          frame_counter+=1\n",
        "          step+=1\n",
        "\n",
        "        rewards.append(episode_reward)\n",
        "\n",
        "        mean_rewards.append(np.mean(np.array(rewards)))\n",
        "        median_rewards.append(np.median(np.array(rewards)))\n",
        "\n",
        "        print(\"Episode {}# Score: {}\".format(episode+1,episode_reward))\n",
        "\n",
        "        # After every step do train from memory\n",
        "        print(\"Training after {} episode.\".format(episode+1))\n",
        "        self.player.replay(self.sample_batch_size)\n",
        "\n",
        "        \n",
        "        print(\"Exploration rate after episode {} is {}\".format(episode+1,self.player.exploration_rate))\n",
        "\n",
        "        if episode_reward>=max_score:\n",
        "          #print(\"Got a high score.\")\n",
        "          max_score=episode_reward\n",
        "\n",
        "        if episode%50==0:\n",
        "          self.player.save_model()\n",
        "\n",
        "          plt.style.use(\"ggplot\")\n",
        "          plt.figure()\n",
        "          plt.plot(np.arange(episode+1),np.array(mean_rewards),label=\"Average score\")\n",
        "          plt.title(\"Episodes and average rewards\")\n",
        "          plt.xlabel(\"Episode #\")\n",
        "          plt.ylabel(\"Average rewards\")\n",
        "          plt.savefig(\"./pacmanmean.png\")\n",
        "\n",
        "\n",
        "          plt.figure()\n",
        "          plt.plot(np.arange(episode+1),np.array(median_rewards),label=\"Median score\")\n",
        "          plt.title(\"Episodes and median rewards\")\n",
        "          plt.xlabel(\"Episode #\")\n",
        "          plt.ylabel(\"Median rewards\")\n",
        "          plt.savefig(\"./pacmanmedian.png\")\n",
        "\n",
        "        #print(\"Now I am going to test my model on testing state...\")\n",
        "      \n",
        "        # After each episode I will test my model on testing states.\n",
        "        avg_score=0   # Avg of maximum cumulative scores I can get on my testing states\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        for test_state in self.player.testing_states:\n",
        "          act_values=self.player.model.predict(test_state)\n",
        "          avg_score+=np.amax(act_values[0])\n",
        "\n",
        "        avg_score=avg_score/len(self.player.testing_states)\n",
        "\n",
        "        print(\"Average score of all testing states is {}\".format(avg_score))\n",
        "\n",
        "        testing_scores.append(avg_score)\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "\n",
        "\n",
        "      print(\"Highest score obtained during training is: {}\".format(max_score))    \n",
        "\n",
        "      \n",
        "\n",
        "      \"\"\"\n",
        "\n",
        "      \n",
        "\n",
        "      \"\"\"\n",
        "\n",
        "  \n",
        "      \n",
        "\n",
        "    finally:\n",
        "      self.player.save_model()\n",
        "      self.env.render(close=True)\n",
        "      \n",
        "  def play(self):\n",
        "\n",
        "    for episode in range(self.episodes):\n",
        "\n",
        "      state=self.env.reset()\n",
        "      state=np.reshape(state,[1,self.state_size])\n",
        "      done=False\n",
        "      step=0\n",
        "\n",
        "      episode_reward=0   # Total reward in an episode\n",
        "\n",
        "      while not done:\n",
        "        self.env.render()\n",
        "        action=self.player.act(state)\n",
        "        next_state,reward,done,_=self.env.step(action)\n",
        "        episode_reward+=reward\n",
        "        next_state=np.reshape(next_state,[1,self.state_size])\n",
        "        state=next_state\n",
        "        step+=1\n",
        "\n",
        "      print(\"Episode {}# Score: {}\".format(episode,episode_reward))\n",
        "\n",
        "    self.env.render(close=True)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GFBjSoTxL9g6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "\tpacman = Pacman()\n",
        "\t#pacman.prepare()\n",
        "\tpacman.learn()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}